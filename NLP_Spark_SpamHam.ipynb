{"cells":[{"cell_type":"markdown","source":["We have tried building a spam detection filter using Python and Spark. Our data set consists of volunteered text messages from a study in Singapore and some spam texts from a UK reporting site. \n\nA spam is an irrelevant or inappropriate message sent on internet to a large group of people. A Ham, on the other hand, is a message which is not a spam and is important to us. \nWe aim to segregate these two messages and classify them as accurately as possible."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Spam-Ham Detection","showTitle":true,"inputWidgets":{},"nuid":"34fa052d-2d6a-4154-93fa-63d8f08f6fc5"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a55a0a2d-1c67-43ac-98e9-af51c8e7c16e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#create a spark session with a relevant name\nspark = SparkSession.builder.appName('nlp').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7f98e3f-f62e-4494-a477-a051f5e4ef5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#read the dataset\ndata = spark.read.csv('dbfs:/FileStore/shared_uploads/sanchari.gautam@gmail.com/SMSSpamCollection',inferSchema=True,sep=\"\\t\")\ndata.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14feed87-ace8-4387-89f3-0d6c1d6b54a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+--------------------+\n| _c0|                 _c1|\n+----+--------------------+\n| ham|Go until jurong p...|\n| ham|Ok lar... Joking ...|\n|spam|Free entry in 2 a...|\n| ham|U dun say so earl...|\n| ham|Nah I don&#39;t think...|\n|spam|FreeMsg Hey there...|\n| ham|Even my brother i...|\n| ham|As per your reque...|\n|spam|WINNER!! As a val...|\n|spam|Had your mobile 1...|\n| ham|I&#39;m gonna be home...|\n|spam|SIX chances to wi...|\n|spam|URGENT! You have ...|\n| ham|I&#39;ve been searchi...|\n| ham|I HAVE A DATE ON ...|\n|spam|XXXMobileMovieClu...|\n| ham|Oh k...i&#39;m watchi...|\n| ham|Eh u remember how...|\n| ham|Fine if thats th...|\n|spam|England v Macedon...|\n+----+--------------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------------------+\n _c0|                 _c1|\n+----+--------------------+\n ham|Go until jurong p...|\n ham|Ok lar... Joking ...|\nspam|Free entry in 2 a...|\n ham|U dun say so earl...|\n ham|Nah I don&#39;t think...|\nspam|FreeMsg Hey there...|\n ham|Even my brother i...|\n ham|As per your reque...|\nspam|WINNER!! As a val...|\nspam|Had your mobile 1...|\n ham|I&#39;m gonna be home...|\nspam|SIX chances to wi...|\nspam|URGENT! You have ...|\n ham|I&#39;ve been searchi...|\n ham|I HAVE A DATE ON ...|\nspam|XXXMobileMovieClu...|\n ham|Oh k...i&#39;m watchi...|\n ham|Eh u remember how...|\n ham|Fine if thats th...|\nspam|England v Macedon...|\n+----+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["As we can see, there is no column name present in our dataset, we would be renaming them as class and text."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e0ea61f-aec9-42fd-9c7a-19cbeac803eb"}}},{"cell_type":"code","source":["#rename the column names\ndata = data.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')\ndata.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cb721ec-8377-4cad-8ab1-eaa27c80604e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----+--------------------+\n|class|                text|\n+-----+--------------------+\n|  ham|Go until jurong p...|\n|  ham|Ok lar... Joking ...|\n| spam|Free entry in 2 a...|\n|  ham|U dun say so earl...|\n|  ham|Nah I don&#39;t think...|\n| spam|FreeMsg Hey there...|\n|  ham|Even my brother i...|\n|  ham|As per your reque...|\n| spam|WINNER!! As a val...|\n| spam|Had your mobile 1...|\n|  ham|I&#39;m gonna be home...|\n| spam|SIX chances to wi...|\n| spam|URGENT! You have ...|\n|  ham|I&#39;ve been searchi...|\n|  ham|I HAVE A DATE ON ...|\n| spam|XXXMobileMovieClu...|\n|  ham|Oh k...i&#39;m watchi...|\n|  ham|Eh u remember how...|\n|  ham|Fine if thats th...|\n| spam|England v Macedon...|\n+-----+--------------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+--------------------+\nclass|                text|\n+-----+--------------------+\n  ham|Go until jurong p...|\n  ham|Ok lar... Joking ...|\n spam|Free entry in 2 a...|\n  ham|U dun say so earl...|\n  ham|Nah I don&#39;t think...|\n spam|FreeMsg Hey there...|\n  ham|Even my brother i...|\n  ham|As per your reque...|\n spam|WINNER!! As a val...|\n spam|Had your mobile 1...|\n  ham|I&#39;m gonna be home...|\n spam|SIX chances to wi...|\n spam|URGENT! You have ...|\n  ham|I&#39;ve been searchi...|\n  ham|I HAVE A DATE ON ...|\n spam|XXXMobileMovieClu...|\n  ham|Oh k...i&#39;m watchi...|\n  ham|Eh u remember how...|\n  ham|Fine if thats th...|\n spam|England v Macedon...|\n+-----+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now, we will be tokenizing each text message into words called tokens. Tokenizer helps us build the models to perform the clsssification. Raw texts are hard to be interpreted and hence splitting them up into smaller units helps us understand the context of the message. \n\nFirst, let us see if there is any pattern that we could find out by a simple analysis. We would be trying to find out the average length of spam and ham text messages.\nLet us find out how to do that!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ca1c9f2-6a84-43e4-af0f-5838dfd197a3"}}},{"cell_type":"code","source":["from pyspark.sql.functions import length"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b2bad07-958c-4933-b0c0-7af44e35f33a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#create a new column \"length\" containing the length of each text message\ndata = data.withColumn('length',length(data['text']))\ndata.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4dadc070-4d0f-473c-b4bf-49d8947eb4a7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----+--------------------+------+\n|class|                text|length|\n+-----+--------------------+------+\n|  ham|Go until jurong p...|   111|\n|  ham|Ok lar... Joking ...|    29|\n| spam|Free entry in 2 a...|   155|\n|  ham|U dun say so earl...|    49|\n|  ham|Nah I don&#39;t think...|    61|\n| spam|FreeMsg Hey there...|   147|\n|  ham|Even my brother i...|    77|\n|  ham|As per your reque...|   160|\n| spam|WINNER!! As a val...|   157|\n| spam|Had your mobile 1...|   154|\n|  ham|I&#39;m gonna be home...|   109|\n| spam|SIX chances to wi...|   136|\n| spam|URGENT! You have ...|   155|\n|  ham|I&#39;ve been searchi...|   196|\n|  ham|I HAVE A DATE ON ...|    35|\n| spam|XXXMobileMovieClu...|   149|\n|  ham|Oh k...i&#39;m watchi...|    26|\n|  ham|Eh u remember how...|    81|\n|  ham|Fine if thats th...|    56|\n| spam|England v Macedon...|   155|\n+-----+--------------------+------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+--------------------+------+\nclass|                text|length|\n+-----+--------------------+------+\n  ham|Go until jurong p...|   111|\n  ham|Ok lar... Joking ...|    29|\n spam|Free entry in 2 a...|   155|\n  ham|U dun say so earl...|    49|\n  ham|Nah I don&#39;t think...|    61|\n spam|FreeMsg Hey there...|   147|\n  ham|Even my brother i...|    77|\n  ham|As per your reque...|   160|\n spam|WINNER!! As a val...|   157|\n spam|Had your mobile 1...|   154|\n  ham|I&#39;m gonna be home...|   109|\n spam|SIX chances to wi...|   136|\n spam|URGENT! You have ...|   155|\n  ham|I&#39;ve been searchi...|   196|\n  ham|I HAVE A DATE ON ...|    35|\n spam|XXXMobileMovieClu...|   149|\n  ham|Oh k...i&#39;m watchi...|    26|\n  ham|Eh u remember how...|    81|\n  ham|Fine if thats th...|    56|\n spam|England v Macedon...|   155|\n+-----+--------------------+------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#computing the average length of spam and ham messages\ndata.groupby('class').mean().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95dd8638-01dd-4ef7-953a-a4de13c166ec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----+-----------------+\n|class|      avg(length)|\n+-----+-----------------+\n|  ham| 71.4545266210897|\n| spam|138.6706827309237|\n+-----+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----------------+\nclass|      avg(length)|\n+-----+-----------------+\n  ham| 71.4545266210897|\n spam|138.6706827309237|\n+-----+-----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import (Tokenizer, StopWordsRemover,CountVectorizer, \n                                IDF, StringIndexer)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91c77614-5f33-4d58-924d-44cf2823a30e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#create an object of the Tokenizer to convert text column into tokens\ntokenizer = Tokenizer(inputCol='text',outputCol='token_text')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0414acb-2040-4067-97b6-660d449afd19"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Stopwords are the most common words in any natural language. These words do not add any value to the over all meaning and context of the document. Hence, it is better to remove these distracting words in order to make the performance of our model better. \n\nSome examples of the stopwords are \"the, is, are, with, and, etc.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f06862f-81b1-4bbd-a859-d3da204e13ea"}}},{"cell_type":"code","source":["#creating an object of the StopWordsRemover to remove stopwords from the column token_text\nstop_remove = StopWordsRemover(inputCol='token_text',outputCol='stop_token')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b3db95c-acd2-481b-8478-8f76d45f8e5d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now, after tokenizing the texts and removing the stopwords, we are only left with those tokens which are important and relevant to our aim of classifying them into spam and ham. \n\nNext, we will try to find out the term frequency of each term or token in the text with the help of CountVectorizer class. Let us see how to do that!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb84a9d0-3ff0-4b99-9d8f-268c01cfed94"}}},{"cell_type":"code","source":["#finding the term frequency of each token \ncountvec = CountVectorizer(inputCol='stop_token',outputCol='c_vec')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30205dae-d7f4-4ede-a77c-e59f1113f351"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Inverse Document Frequency is a statistical weight to measure the importance of a term in a collection of documents. The more usage a term has across documents, the less relevant it is to the particular document concerned. \n\nWe would be computing the IDF of each token across all messages to find out their significance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fa33f5e-52de-4e43-a781-fac61fd2ee7d"}}},{"cell_type":"code","source":["#finding the IDF and saving it the tf_idf column\nidf = IDF(inputCol='c_vec',outputCol='tf_idf')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3cbb933-14ff-40b8-ac4e-d47e56aa2aa7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now, in PySpark, we have a disadvantage of not being able to use classification labels as strings. So, we need to convert them into indexers to feed them to the model.\n\nWe would be using the StringIndexer to perform this operation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a15ae71-4b2a-4f14-aebe-5db81af8bc14"}}},{"cell_type":"code","source":["#converts ham/spam to 0/1 format\nham_spam_numeric = StringIndexer(inputCol='class',outputCol='label')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bc8550f-b296-47bc-baa1-5dd8800725e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38811bd7-1d7d-410a-bc32-5f7ec70a47af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Finally, we are in the stage where we have all the columns required in the appropriate format. The final straw is in the conversion of all the columns into one dense vector to make it libsvm compatible."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60abfe91-5eee-40da-8cfc-dbf2a4fd78cd"}}},{"cell_type":"code","source":["#create the features column having the needed columns in a dense vector format\nclean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ed229c0-ff34-46c9-ad0e-90aaa6a826c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now that we have the needed dataframe with two columns - features and class, we are ready to build our classification model. Here, we would be using the Naive Bayes' model for the same."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78106112-e07d-42f8-94be-0a87164dccf1"}}},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d167e600-ed34-4a1f-a9a1-30218849bdb7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#creating a Naive Bayes model object with the default parameters\nnb = NaiveBayes()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8806cc5-b039-449f-ba69-e5a8fba4c30e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Finally, we would be performing all the operations in a sequence on the original dataset to make it memory efficient and less time consuming."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad6b9395-c341-4cf9-b5b5-e7de9e839132"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6861afd-0595-41bb-a62e-857d69ba7801"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#creating the pipeline object with the stages in the sequential manner\ndata_prep_pipe = Pipeline(stages=[ham_spam_numeric,tokenizer,stop_remove,countvec,idf,clean_up])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2dd54dc-9d01-4ad3-a780-cf736294172c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Fitting the pipeline object with our data\ncleaner = data_prep_pipe.fit(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10b455b3-97ac-491e-a480-1503cb78f38b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Transform the original dataset \nclean_data = cleaner.transform(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b0a05e0-ed52-47be-ac94-37802973962c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#select only the label and features column  \nfinal_data = clean_data.select('label','features')\nfinal_data.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"359e79b7-b468-40d4-b39b-7f44183e4d6d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----+--------------------+\n|label|            features|\n+-----+--------------------+\n|  0.0|(13424,[7,11,31,6...|\n|  0.0|(13424,[0,24,297,...|\n|  1.0|(13424,[2,13,19,3...|\n|  0.0|(13424,[0,70,80,1...|\n|  0.0|(13424,[36,134,31...|\n|  1.0|(13424,[10,60,139...|\n|  0.0|(13424,[10,53,103...|\n|  0.0|(13424,[125,184,4...|\n|  1.0|(13424,[1,47,118,...|\n|  1.0|(13424,[0,1,13,27...|\n|  0.0|(13424,[18,43,120...|\n|  1.0|(13424,[8,17,37,8...|\n|  1.0|(13424,[13,30,47,...|\n|  0.0|(13424,[39,96,217...|\n|  0.0|(13424,[552,1697,...|\n|  1.0|(13424,[30,109,11...|\n|  0.0|(13424,[82,214,47...|\n|  0.0|(13424,[0,2,49,13...|\n|  0.0|(13424,[0,74,105,...|\n|  1.0|(13424,[4,30,33,5...|\n+-----+--------------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+--------------------+\nlabel|            features|\n+-----+--------------------+\n  0.0|(13424,[7,11,31,6...|\n  0.0|(13424,[0,24,297,...|\n  1.0|(13424,[2,13,19,3...|\n  0.0|(13424,[0,70,80,1...|\n  0.0|(13424,[36,134,31...|\n  1.0|(13424,[10,60,139...|\n  0.0|(13424,[10,53,103...|\n  0.0|(13424,[125,184,4...|\n  1.0|(13424,[1,47,118,...|\n  1.0|(13424,[0,1,13,27...|\n  0.0|(13424,[18,43,120...|\n  1.0|(13424,[8,17,37,8...|\n  1.0|(13424,[13,30,47,...|\n  0.0|(13424,[39,96,217...|\n  0.0|(13424,[552,1697,...|\n  1.0|(13424,[30,109,11...|\n  0.0|(13424,[82,214,47...|\n  0.0|(13424,[0,2,49,13...|\n  0.0|(13424,[0,74,105,...|\n  1.0|(13424,[4,30,33,5...|\n+-----+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Model Building and training are done now on 80% of the dataset and it is tested against 20% of the dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37399441-442e-4137-a375-93c5e5cfb3e5"}}},{"cell_type":"code","source":["#test train split\ntrain,test = final_data.randomSplit([0.8,0.2])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25af5cb9-62c8-4bb8-9903-637596fabdef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#train the Naive Bayes model\nspam_detector = nb.fit(train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2699365d-03ab-404c-8416-a54d99624494"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Predict on the test dataset\ntest_results = spam_detector.transform(test)\ntest_results.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fe72156-896f-4d2c-943e-b2ef314427f8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----+--------------------+--------------------+--------------------+----------+\n|label|            features|       rawPrediction|         probability|prediction|\n+-----+--------------------+--------------------+--------------------+----------+\n|  0.0|(13424,[0,1,2,7,8...|[-792.38448252351...|[1.0,6.6562709905...|       0.0|\n|  0.0|(13424,[0,1,5,20,...|[-803.66953181862...|[1.0,1.3296035501...|       0.0|\n|  0.0|(13424,[0,1,14,18...|[-1373.0612011283...|[1.0,6.8483363421...|       0.0|\n|  0.0|(13424,[0,1,15,20...|[-676.94181415862...|[1.0,8.2292105524...|       0.0|\n|  0.0|(13424,[0,1,17,19...|[-806.30285256101...|[1.0,5.0408693534...|       0.0|\n|  0.0|(13424,[0,1,24,31...|[-343.07651460777...|[1.0,1.5546896402...|       0.0|\n|  0.0|(13424,[0,1,27,35...|[-1479.7481923014...|[0.99999999999998...|       0.0|\n|  0.0|(13424,[0,2,4,5,7...|[-988.59500667556...|[1.0,4.5220183940...|       0.0|\n|  0.0|(13424,[0,2,4,5,1...|[-1618.6251418899...|[1.0,5.5692797648...|       0.0|\n|  0.0|(13424,[0,2,4,7,2...|[-516.31084523058...|[1.0,1.3602106851...|       0.0|\n|  0.0|(13424,[0,2,4,10,...|[-1228.5329583926...|[1.0,6.4805499018...|       0.0|\n|  0.0|(13424,[0,2,4,25,...|[-440.78238607243...|[1.0,3.6971028161...|       0.0|\n|  0.0|(13424,[0,2,4,40,...|[-1593.9798208678...|[0.99961990007688...|       0.0|\n|  0.0|(13424,[0,2,7,8,1...|[-688.12935023614...|[1.0,7.2345025488...|       0.0|\n|  0.0|(13424,[0,2,7,11,...|[-1341.9303396725...|[1.0,1.1581876407...|       0.0|\n|  0.0|(13424,[0,2,7,31,...|[-671.54148869120...|[1.0,2.9557336467...|       0.0|\n|  0.0|(13424,[0,2,11,13...|[-1398.6780646412...|[1.0,4.6058060702...|       0.0|\n|  0.0|(13424,[0,2,14,24...|[-431.20302497589...|[0.99999999999997...|       0.0|\n|  0.0|(13424,[0,2,20,60...|[-1231.7729839481...|[1.0,2.8380194435...|       0.0|\n|  0.0|(13424,[0,2,20,10...|[-383.94501479769...|[1.0,1.8518971207...|       0.0|\n+-----+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+--------------------+--------------------+--------------------+----------+\nlabel|            features|       rawPrediction|         probability|prediction|\n+-----+--------------------+--------------------+--------------------+----------+\n  0.0|(13424,[0,1,2,7,8...|[-792.38448252351...|[1.0,6.6562709905...|       0.0|\n  0.0|(13424,[0,1,5,20,...|[-803.66953181862...|[1.0,1.3296035501...|       0.0|\n  0.0|(13424,[0,1,14,18...|[-1373.0612011283...|[1.0,6.8483363421...|       0.0|\n  0.0|(13424,[0,1,15,20...|[-676.94181415862...|[1.0,8.2292105524...|       0.0|\n  0.0|(13424,[0,1,17,19...|[-806.30285256101...|[1.0,5.0408693534...|       0.0|\n  0.0|(13424,[0,1,24,31...|[-343.07651460777...|[1.0,1.5546896402...|       0.0|\n  0.0|(13424,[0,1,27,35...|[-1479.7481923014...|[0.99999999999998...|       0.0|\n  0.0|(13424,[0,2,4,5,7...|[-988.59500667556...|[1.0,4.5220183940...|       0.0|\n  0.0|(13424,[0,2,4,5,1...|[-1618.6251418899...|[1.0,5.5692797648...|       0.0|\n  0.0|(13424,[0,2,4,7,2...|[-516.31084523058...|[1.0,1.3602106851...|       0.0|\n  0.0|(13424,[0,2,4,10,...|[-1228.5329583926...|[1.0,6.4805499018...|       0.0|\n  0.0|(13424,[0,2,4,25,...|[-440.78238607243...|[1.0,3.6971028161...|       0.0|\n  0.0|(13424,[0,2,4,40,...|[-1593.9798208678...|[0.99961990007688...|       0.0|\n  0.0|(13424,[0,2,7,8,1...|[-688.12935023614...|[1.0,7.2345025488...|       0.0|\n  0.0|(13424,[0,2,7,11,...|[-1341.9303396725...|[1.0,1.1581876407...|       0.0|\n  0.0|(13424,[0,2,7,31,...|[-671.54148869120...|[1.0,2.9557336467...|       0.0|\n  0.0|(13424,[0,2,11,13...|[-1398.6780646412...|[1.0,4.6058060702...|       0.0|\n  0.0|(13424,[0,2,14,24...|[-431.20302497589...|[0.99999999999997...|       0.0|\n  0.0|(13424,[0,2,20,60...|[-1231.7729839481...|[1.0,2.8380194435...|       0.0|\n  0.0|(13424,[0,2,20,10...|[-383.94501479769...|[1.0,1.8518971207...|       0.0|\n+-----+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79c6cb25-1cc6-4d5d-80f8-6badb6bddc8d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#evaluating the prediction of the spam and ham on the test dataset\nacc_eval = MulticlassClassificationEvaluator()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"170013ba-85d0-4d3a-9007-be2127f5072b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#computing the accuracy\nacc = acc_eval.evaluate(test_results)\nacc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67fc42f4-84e3-433f-9636-379b674cb07e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[28]: 0.9194453777008199</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[28]: 0.9194453777008199</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Thus we can see how raw texts can be fed into mathematical machine learning models for classification. Since by using the default parameters of the Naive Bayes model, we have achieved a near perfect model with an accuracy of 93%, we can conclude that our model is good to go for the classification."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6ea0898-e783-4d4b-a867-3b1a0fd1824f"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"NLP_Spark_SpamHam","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2273222872076485}},"nbformat":4,"nbformat_minor":0}
